{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is TensorFlow?\n",
    "---\n",
    "TensorFlow is an open-source machine learning framework developed by Google. It provides a comprehensive ecosystem of tools, libraries, and community resources to develop, train, and deploy machine learning and deep learning models. TensorFlow supports a wide range of tasks, including image recognition, natural language processing (NLP), and time-series analysis.\n",
    "\n",
    "What is TensorFlow used for?\n",
    "---\n",
    "Deep Learning: Build and train neural networks for tasks like image classification, object detection, or speech recognition.\n",
    "\n",
    "Machine Learning: Implement traditional algorithms such as linear regression, clustering, and decision trees.\n",
    "\n",
    "Production Deployment: Optimize and serve models efficiently on mobile, edge devices, or cloud platforms.\n",
    "\n",
    "Research: TensorFlow is widely used in AI research due to its flexibility and scalability.\n",
    "\n",
    "Key Features:\n",
    "---\n",
    "TensorFlow.js: For running machine learning in the browser.\n",
    "\n",
    "TensorFlow Lite: For deploying on mobile and embedded devices.\n",
    "\n",
    "Keras Integration: Offers a user-friendly API for building and training models.\n",
    "\n",
    "TensorBoard: Visualize training progress and performance metrics.\n",
    "\n",
    "Now I am using it to train a handwriting digit recognization model with MNIST database\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models # type: ignore I don't know why it reports errors but the code still can run\n",
    "from keras.datasets import mnist # type: ignore\n",
    "from keras.utils import to_categorical # type: ignore\n",
    "from keras.models import load_model  # type: ignore\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import ImageEnhance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset and spilt it to training set and validation set\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape to (28, 28, 1) because CNN expects a 3D input: (height, width, channels)\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the CNN model with 3 convolutional layers and maxpooling layers inbetween\n",
    "---\n",
    "\n",
    "CNNs are excellent for image processing due to their efficient handling of spatial information, ability to learn hierarchical features, and reduced computational complexity. This makes them the backbone of modern applications like object detection, facial recognition, and image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the first convolutional layer with 32 filters and ReLU activation\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# Add the first max pooling layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Add the second convolutional layer with 64 filters\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add the second max pooling layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Add the third convolutional layer with 64 filters\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Flatten the feature maps to prepare for the fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Add a fully connected layer with 64 units and ReLU activation\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Add the output layer with 10 units (one for each digit) and softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model\n",
    "---\n",
    "with Adam Optimizer and categorical crossentropy as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.5471 - accuracy: 0.8483 - val_loss: 0.1747 - val_accuracy: 0.9484\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1509 - accuracy: 0.9557 - val_loss: 0.1075 - val_accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.1083 - accuracy: 0.9675 - val_loss: 0.0780 - val_accuracy: 0.9768\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0869 - accuracy: 0.9735 - val_loss: 0.0681 - val_accuracy: 0.9785\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0733 - accuracy: 0.9782 - val_loss: 0.0569 - val_accuracy: 0.9820\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0637 - accuracy: 0.9804 - val_loss: 0.0544 - val_accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0574 - accuracy: 0.9822 - val_loss: 0.0500 - val_accuracy: 0.9831\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.0457 - val_accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0467 - accuracy: 0.9860 - val_loss: 0.0429 - val_accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0425 - accuracy: 0.9869 - val_loss: 0.0368 - val_accuracy: 0.9869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3d6a39bb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training data\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model's performance\n",
    "---\n",
    "\n",
    "This model performs well on training and validation sets, but it seems to be bad with my own handwriting.\n",
    "\n",
    "This could be due to OVERFITTING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0368 - accuracy: 0.9869\n",
      "Test accuracy: 0.9868999719619751\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test dataset\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model.save(r\"Models_Trained/cnn_mnist_CLR.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are testing with my own handwriting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = load_model(r\"Models_Trained/cnn_mnist_CLR.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory where the images are stored\n",
    "image_dir = r\"Pics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all image file paths in the directory (assuming .png format) and sort them\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nothing but failure\n",
    "---\n",
    "Probably because of the overfitting issue\n",
    "\n",
    "Or the testing pictures are not normalized properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "0.png: The model predicts this digit is 8.\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1.png: The model predicts this digit is 0.\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "2.png: The model predicts this digit is 2.\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "3.png: The model predicts this digit is 8.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "4.png: The model predicts this digit is 8.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "5.png: The model predicts this digit is 8.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "6.png: The model predicts this digit is 8.\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "7.png: The model predicts this digit is 8.\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "8.png: The model predicts this digit is 2.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "9.png: The model predicts this digit is 8.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each image file in sorted order\n",
    "for image_file in image_files:\n",
    "    # Load the image and convert to grayscale\n",
    "    image = Image.open(os.path.join(image_dir, image_file)).convert('L')\n",
    "\n",
    "    # Resize image (28x28) and choose resampling filter (NEAREST)\n",
    "    image = image.resize((28, 28), Image.NEAREST)\n",
    "\n",
    "    # Convert to Numpy Array and Normalize to [0, 1]\n",
    "    image = np.array(image).astype('float32') / 255\n",
    "\n",
    "    # Reshape to (1, 28, 28, 1) to match the input shape expected by the CNN model\n",
    "    image = image.reshape(1, 28, 28, 1)\n",
    "\n",
    "     # Make prediction\n",
    "    prediction = model.predict(image)\n",
    "    predicted_digit = np.argmax(prediction)\n",
    "\n",
    "    # Output the filename and prediction\n",
    "    print(f'{image_file}: The model predicts this digit is {predicted_digit}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
